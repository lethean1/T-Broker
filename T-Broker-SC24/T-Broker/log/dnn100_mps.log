Roberta_batch_32_samples_8000_seed_42
04/11/2024 15:30:53 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:30:53 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-30-53_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:30:53 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
T5_batch_7_samples_1500_seed_42
04/11/2024 15:30:53 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:30:53 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_15-30-53_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=7,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:32:33 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:32:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_7_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
04/11/2024 15:32:34 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:32:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
04/11/2024 15:32:34 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:32:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 395.20it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_32_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Roberta_batch_32_samples_8000_seed_42
04/11/2024 15:32:46 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:32:46 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-32-46_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:32:46 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
TC_batch_32_samples_25000_seed_42
04/11/2024 15:32:46 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:32:46 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-32-46_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:34:26 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:34:26 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:34:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:34:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:34:26 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:34:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 447.49it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_32_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_32_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Bert_batch_5_samples_5000_seed_42
[BertQA] Init parsers
[BertQA] parsing ['--model_name_or_path', 'bert-base-uncased', '--dataset_name', 'squad', '--do_train', '--per_device_train_batch_size', '5', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--doc_stride', '128', '--max_train_samples', '5000', '--warmup_iters', '0', '--memory_buffer', '2', '--data_seed', '42', '--memory_threshold', '31', '--output_dir', '/tmp/test_squad/', '--overwrite_output_dir']
[BertQA] argsv is ['mps_main.py', '/home/T-Broker-SC24/T-Broker/data/task_queues/dnn100.txt', '5']
[BertQA] Done parsing
[BertQA] Done copying
04/11/2024 15:34:38 - WARNING - task.question_answering_main_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:34:38 - INFO - task.question_answering_main_qa - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test_squad/runs/Apr11_15-34-38_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test_squad/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=5,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test_squad/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:34:38 - INFO - task.question_answering_main_qa - DataTraining parameters DataTrainingArguments(dataset_name='squad', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=5000, max_eval_samples=None, max_predict_samples=None, version_2_with_negative=False, null_score_diff_threshold=0.0, doc_stride=128, n_best_size=20, max_answer_length=30, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False)
TC_batch_8_samples_25000_seed_42
04/11/2024 15:34:38 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:34:38 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-34-38_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_batch_5_samples_5000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/question_answering_main_qa.py", line 308, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'squad' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_8_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
Bert_batch_30_samples_5000_seed_42
[BertQA] Init parsers
[BertQA] parsing ['--model_name_or_path', 'bert-base-uncased', '--dataset_name', 'squad', '--do_train', '--per_device_train_batch_size', '30', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--doc_stride', '128', '--max_train_samples', '5000', '--warmup_iters', '0', '--memory_buffer', '2', '--data_seed', '42', '--memory_threshold', '31', '--output_dir', '/tmp/test_squad/', '--overwrite_output_dir']
[BertQA] argsv is ['mps_main.py', '/home/T-Broker-SC24/T-Broker/data/task_queues/dnn100.txt', '6']
[BertQA] Done parsing
[BertQA] Done copying
04/11/2024 15:36:20 - WARNING - task.question_answering_main_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:36:20 - INFO - task.question_answering_main_qa - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test_squad/runs/Apr11_15-36-20_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test_squad/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=30,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test_squad/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:36:20 - INFO - task.question_answering_main_qa - DataTraining parameters DataTrainingArguments(dataset_name='squad', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=5000, max_eval_samples=None, max_predict_samples=None, version_2_with_negative=False, null_score_diff_threshold=0.0, doc_stride=128, n_best_size=20, max_answer_length=30, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False)
Roberta_batch_40_samples_8000_seed_42
04/11/2024 15:36:20 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:36:20 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-36-20_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=40,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:36:20 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_batch_30_samples_5000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/question_answering_main_qa.py", line 308, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'squad' on the Hub (ConnectionError)
04/11/2024 15:38:00 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:38:00 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:38:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:38:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:38:00 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:38:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 437.62it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_40_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Roberta_batch_8_samples_8000_seed_42
04/11/2024 15:38:12 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:38:12 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-38-12_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:38:12 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
Roberta_batch_8_samples_8000_seed_42
04/11/2024 15:38:12 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:38:12 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-38-12_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:38:12 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
04/11/2024 15:39:53 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:39:53 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:39:53 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:39:53 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:39:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:39:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:39:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:39:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:39:53 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:39:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]04/11/2024 15:39:53 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:39:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 402.06it/s]
100%|██████████| 3/3 [00:00<00:00, 423.07it/s]
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_8_samples_8000_seed_42.py", line 8, in import_task
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_8_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

T5_batch_7_samples_1500_seed_42
04/11/2024 15:40:05 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:40:05 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_15-40-05_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=7,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
TC_batch_40_samples_25000_seed_42
04/11/2024 15:40:05 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:40:05 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-40-05_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=40,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_7_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_40_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
Bert_batch_5_samples_5000_seed_42
[BertQA] Init parsers
[BertQA] parsing ['--model_name_or_path', 'bert-base-uncased', '--dataset_name', 'squad', '--do_train', '--per_device_train_batch_size', '5', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--doc_stride', '128', '--max_train_samples', '5000', '--warmup_iters', '0', '--memory_buffer', '2', '--data_seed', '42', '--memory_threshold', '31', '--output_dir', '/tmp/test_squad/', '--overwrite_output_dir']
[BertQA] argsv is ['mps_main.py', '/home/T-Broker-SC24/T-Broker/data/task_queues/dnn100.txt', '12']
[BertQA] Done parsing
[BertQA] Done copying
04/11/2024 15:41:47 - WARNING - task.question_answering_main_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:41:47 - INFO - task.question_answering_main_qa - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test_squad/runs/Apr11_15-41-47_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test_squad/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=5,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test_squad/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:41:47 - INFO - task.question_answering_main_qa - DataTraining parameters DataTrainingArguments(dataset_name='squad', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=5000, max_eval_samples=None, max_predict_samples=None, version_2_with_negative=False, null_score_diff_threshold=0.0, doc_stride=128, n_best_size=20, max_answer_length=30, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False)
Roberta_batch_16_samples_8000_seed_42
04/11/2024 15:41:47 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:41:47 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-41-47_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:41:47 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
04/11/2024 15:43:27 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:43:27 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_batch_5_samples_5000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/question_answering_main_qa.py", line 308, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'squad' on the Hub (ConnectionError)
04/11/2024 15:43:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:43:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:43:27 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:43:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 445.07it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_16_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
T5_batch_4_samples_1500_seed_42
04/11/2024 15:43:39 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:43:39 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_15-43-39_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
T5_batch_4_samples_1500_seed_42
04/11/2024 15:43:39 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:43:39 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_15-43-39_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_4_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_4_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
Roberta_batch_32_samples_8000_seed_42
04/11/2024 15:45:22 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:45:22 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-45-22_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:45:22 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
TC_batch_40_samples_25000_seed_42
04/11/2024 15:45:22 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:45:22 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-45-22_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=40,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:47:02 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:47:02 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_40_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
04/11/2024 15:47:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:47:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:47:02 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:47:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 400.63it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_32_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
T5_batch_4_samples_1500_seed_42
04/11/2024 15:47:14 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:47:14 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_15-47-14_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Bert_batch_10_samples_5000_seed_42
[BertQA] Init parsers
[BertQA] parsing ['--model_name_or_path', 'bert-base-uncased', '--dataset_name', 'squad', '--do_train', '--per_device_train_batch_size', '10', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--doc_stride', '128', '--max_train_samples', '5000', '--warmup_iters', '0', '--memory_buffer', '2', '--data_seed', '42', '--memory_threshold', '31', '--output_dir', '/tmp/test_squad/', '--overwrite_output_dir']
[BertQA] argsv is ['mps_main.py', '/home/T-Broker-SC24/T-Broker/data/task_queues/dnn100.txt', '19']
[BertQA] Done parsing
[BertQA] Done copying
04/11/2024 15:47:14 - WARNING - task.question_answering_main_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:47:14 - INFO - task.question_answering_main_qa - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test_squad/runs/Apr11_15-47-14_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test_squad/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=10,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test_squad/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:47:14 - INFO - task.question_answering_main_qa - DataTraining parameters DataTrainingArguments(dataset_name='squad', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=5000, max_eval_samples=None, max_predict_samples=None, version_2_with_negative=False, null_score_diff_threshold=0.0, doc_stride=128, n_best_size=20, max_answer_length=30, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_4_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_batch_10_samples_5000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/question_answering_main_qa.py", line 308, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'squad' on the Hub (ConnectionError)
