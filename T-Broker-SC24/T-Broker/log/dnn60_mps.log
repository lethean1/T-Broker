Bert_batch_30_samples_5000_seed_42
[BertQA] Init parsers
[BertQA] parsing ['--model_name_or_path', 'bert-base-uncased', '--dataset_name', 'squad', '--do_train', '--per_device_train_batch_size', '30', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--doc_stride', '128', '--max_train_samples', '5000', '--warmup_iters', '0', '--memory_buffer', '2', '--data_seed', '42', '--memory_threshold', '31', '--output_dir', '/tmp/test_squad/', '--overwrite_output_dir']
[BertQA] argsv is ['mps_main.py', '/home/T-Broker-SC24/T-Broker/data/task_queues/dnn60.txt', '1']
[BertQA] Done parsing
[BertQA] Done copying
Roberta_batch_8_samples_8000_seed_42
04/11/2024 14:57:19 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 14:57:19 - WARNING - task.question_answering_main_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 14:57:19 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_14-57-19_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 14:57:19 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
04/11/2024 14:57:19 - INFO - task.question_answering_main_qa - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test_squad/runs/Apr11_14-57-19_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test_squad/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=30,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test_squad/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 14:57:19 - INFO - task.question_answering_main_qa - DataTraining parameters DataTrainingArguments(dataset_name='squad', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=5000, max_eval_samples=None, max_predict_samples=None, version_2_with_negative=False, null_score_diff_threshold=0.0, doc_stride=128, n_best_size=20, max_answer_length=30, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
04/11/2024 14:58:59 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_batch_30_samples_5000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/question_answering_main_qa.py", line 308, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'squad' on the Hub (ConnectionError)
04/11/2024 14:58:59 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 14:58:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 14:58:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 14:58:59 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 14:58:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 400.04it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_8_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
TIMESTAMP, [JOB][BEGIN], [GCN_dataset_cora_num_layers_5_4000]-3 begin, 1712847551.380450
T5_batch_4_samples_1500_seed_42
04/11/2024 14:59:11 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 14:59:11 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_14-59-11_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
TIMESTAMP, [JOB][END], [GCN_dataset_cora_num_layers_5_4000]-3 finished, 1712847575.276836
GCN_dataset_cora_num_layers_5_4000
MainThread GCN training cora >>>>>>>>>> 1712847551.3804946
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_4_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
Roberta_batch_8_samples_8000_seed_42
04/11/2024 15:00:54 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:00:54 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-00-54_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:00:54 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
TC_batch_40_samples_25000_seed_42
04/11/2024 15:00:54 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:00:54 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-00-54_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=40,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:02:34 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:02:34 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:02:34 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:02:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:02:34 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:02:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 397.09it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_40_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_8_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Roberta_batch_40_samples_8000_seed_42
04/11/2024 15:02:46 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:02:46 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-02-46_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=40,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:02:46 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
TIMESTAMP, [JOB][BEGIN], [GraphSAGE_dataset_Yeast_num_layers_6_400]-6 begin, 1712847766.623755
TIMESTAMP, [JOB][END], [GraphSAGE_dataset_Yeast_num_layers_6_400]-6 finished, 1712847859.779845
GraphSAGE_dataset_Yeast_num_layers_6_400
MainThread GraphSAGE training Yeast >>>>>>>>>> 1712847766.6238182
04/11/2024 15:04:26 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:04:26 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:04:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:04:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:04:26 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:04:26 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 401.42it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_40_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
TIMESTAMP, [JOB][BEGIN], [GCN_dataset_cora_num_layers_6_4000]-8 begin, 1712847878.191943
TIMESTAMP, [JOB][BEGIN], [GIN_dataset_citeseer_num_layers_6_4000]-9 begin, 1712847878.325641
TIMESTAMP, [JOB][END], [GCN_dataset_cora_num_layers_6_4000]-8 finished, 1712847911.032328
GCN_dataset_cora_num_layers_6_4000
MainThread GCN training cora >>>>>>>>>> 1712847878.1919897
TIMESTAMP, [JOB][END], [GIN_dataset_citeseer_num_layers_6_4000]-9 finished, 1712847912.452114
GIN_dataset_citeseer_num_layers_6_4000
MainThread GIN training citeseer >>>>>>>>>> 1712847878.3256974
before training
time:34.01808953285217
TIMESTAMP, [JOB][BEGIN], [GIN_dataset_citeseer_num_layers_5_4000]-11 begin, 1712847914.188070
TC_batch_32_samples_25000_seed_42
04/11/2024 15:05:14 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:05:14 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-05-14_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
TIMESTAMP, [JOB][END], [GIN_dataset_citeseer_num_layers_5_4000]-11 finished, 1712847943.829065
GIN_dataset_citeseer_num_layers_5_4000
MainThread GIN training citeseer >>>>>>>>>> 1712847914.1881218
before training
time:29.55033779144287
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_32_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
Bert_batch_20_samples_5000_seed_42
[BertQA] Init parsers
[BertQA] parsing ['--model_name_or_path', 'bert-base-uncased', '--dataset_name', 'squad', '--do_train', '--per_device_train_batch_size', '20', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--doc_stride', '128', '--max_train_samples', '5000', '--warmup_iters', '0', '--memory_buffer', '2', '--data_seed', '42', '--memory_threshold', '31', '--output_dir', '/tmp/test_squad/', '--overwrite_output_dir']
[BertQA] argsv is ['mps_main.py', '/home/T-Broker-SC24/T-Broker/data/task_queues/dnn60.txt', '12']
[BertQA] Done parsing
[BertQA] Done copying
04/11/2024 15:06:56 - WARNING - task.question_answering_main_qa - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:06:56 - INFO - task.question_answering_main_qa - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test_squad/runs/Apr11_15-06-56_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test_squad/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=20,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test_squad/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:06:56 - INFO - task.question_answering_main_qa - DataTraining parameters DataTrainingArguments(dataset_name='squad', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=5000, max_eval_samples=None, max_predict_samples=None, version_2_with_negative=False, null_score_diff_threshold=0.0, doc_stride=128, n_best_size=20, max_answer_length=30, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False)
TC_batch_8_samples_25000_seed_42
04/11/2024 15:06:57 - WARNING - task.text_classification_main_glue - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:06:57 - INFO - task.text_classification_main_glue - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/$TASK_NAME/runs/Apr11_15-06-57_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/$TASK_NAME/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/$TASK_NAME/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_batch_20_samples_5000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Bert_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/question_answering_main_qa.py", line 308, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'squad' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_batch_8_samples_25000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/TC_base.py", line 10, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/text_classification_main_glue.py", line 297, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'glue' on the Hub (ConnectionError)
TIMESTAMP, [JOB][BEGIN], [GCN_dataset_cora_num_layers_7_4000]-15 begin, 1712848118.902129
TIMESTAMP, [JOB][BEGIN], [GIN_dataset_citeseer_num_layers_3_4000]-14 begin, 1712848118.968967
TIMESTAMP, [JOB][END], [GIN_dataset_citeseer_num_layers_3_4000]-14 finished, 1712848138.915464
GIN_dataset_citeseer_num_layers_3_4000
MainThread GIN training citeseer >>>>>>>>>> 1712848118.9690204
before training
time:19.8447105884552
TIMESTAMP, [JOB][END], [GCN_dataset_cora_num_layers_7_4000]-15 finished, 1712848154.594687
GCN_dataset_cora_num_layers_7_4000
MainThread GCN training cora >>>>>>>>>> 1712848118.9021752
TIMESTAMP, [JOB][BEGIN], [GIN_dataset_citeseer_num_layers_7_4000]-16 begin, 1712848156.332367
Roberta_batch_8_samples_8000_seed_42
04/11/2024 15:09:16 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:09:16 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-09-16_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:09:16 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
TIMESTAMP, [JOB][END], [GIN_dataset_citeseer_num_layers_7_4000]-16 finished, 1712848198.337775
GIN_dataset_citeseer_num_layers_7_4000
MainThread GIN training citeseer >>>>>>>>>> 1712848156.3324196
before training
time:41.87868285179138
04/11/2024 15:10:56 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:10:56 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:10:56 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:10:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:10:56 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:10:56 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 445.48it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_8_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
Roberta_batch_32_samples_8000_seed_42
04/11/2024 15:11:09 - WARNING - task.MultipleChoice - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:11:09 - INFO - task.MultipleChoice - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/test-swag-no-trainer/runs/Apr11_15-11-09_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/test-swag-no-trainer,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/test-swag-no-trainer,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:11:09 - INFO - task.MultipleChoice - DataTraining parameters DataTrainingArguments(train_file=None, validation_file=None, overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=None, pad_to_max_length=False, max_train_samples=8000, max_eval_samples=None, dynamic_checkpoint=True, warmup_iters=0, static_checkpoint=False, max_input_size=142, min_input_size=32, prev_checkpoint=False, profile_memory=False, only_input_size=False, memory_predict_test=False)
T5_batch_7_samples_1500_seed_42
04/11/2024 15:11:09 - WARNING - task.translation_main - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/11/2024 15:11:09 - INFO - task.translation_main - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/tmp/t5-base-un/runs/Apr11_15-11-09_gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
memory_buffer=2.0,
memory_threshold=31.0,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/tmp/t5-base-un,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=7,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/t5-base-un,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_dtr=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
04/11/2024 15:12:49 - WARNING - datasets.load - Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Thu May 25 15:54:53 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.
04/11/2024 15:12:49 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:12:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
04/11/2024 15:12:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
04/11/2024 15:12:49 - WARNING - datasets.builder - Found cached dataset swag (/root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)
04/11/2024 15:12:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 455.41it/s]
Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_batch_7_samples_1500_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/T5_base.py", line 11, in import_fn
    runner = TransformerLoader(main,"",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/translation_main.py", line 336, in main
    raw_datasets = load_dataset(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/home/sqx/anaconda3/envs/mps/lib/python3.9/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'un_pc' on the Hub (ConnectionError)
Traceback (most recent call last):
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 596, in _get_config_dict
    resolved_config_file = cached_path(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 282, in cached_path
    output_path = get_from_cache(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/utils/hub.py", line 545, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 48, in <module>
    main()
  File "/home/T-Broker-SC24/T-Broker/software_cognn/mps/mps_main.py", line 39, in main
    model, func, _ = model_module.import_task(data_name, num_layers)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_batch_32_samples_8000_seed_42.py", line 8, in import_task
    return import_fn( **{
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/Roberta_base.py", line 10, in import_fn
    runner = TransformerLoader(main, "",[
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/transformer_loader.py", line 6, in __init__
    self.model = next(self.generator)
  File "/home/T-Broker-SC24/T-Broker/software_cognn/task/MultipleChoice.py", line 346, in main
    config = AutoConfig.from_pretrained(
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/models/auto/configuration_auto.py", line 652, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 548, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/T-Broker-SC24/mimose-transformers/src/transformers/configuration_utils.py", line 629, in _get_config_dict
    raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like roberta-base is not the path to a directory containing a {configuration_file} file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
